{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164be129",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from cnn_utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ea7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ec94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "argtrain = 'y'\n",
    "argepoch = 100\n",
    "argmodel = 'alexnet'\n",
    "arglabel = 'flower_to_name.json'\n",
    "argdir = os.path.expanduser('~') + '/Programming Data/Flower_data/'\n",
    "\n",
    "# 'vgg', 'alexnet', 'googlenet', 'densenet', 'inception', 'resnext', 'shufflenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c90468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From cnn_operational_functions.py import *\n",
    "\n",
    "def o1_train_model(model, dict_data_loaders, epoch, type_loader, criterion):\n",
    "\n",
    "    print(\"Using GPU\" if torch.cuda.is_available() else \"WARNING\")\n",
    "    t0 = time.time() # initialize start time for running training\n",
    "\n",
    "    running_count = 0 # initialize running count in order to track number of epochs fine tuning deeper network\n",
    "    running = False # initialize running variable to start system with deeper network frozen\n",
    "\n",
    "    # Define default hyperparameters: learning rate and weight decay\n",
    "    model_hyperparameters = {'learnrate': 0.003,\n",
    "                         'training_loss_history': [],\n",
    "                         'validate_loss_history': [],\n",
    "                         'epoch_on': [],\n",
    "                         'running_count': 0,\n",
    "                         'weightdecay' : 0.00001}\n",
    "\n",
    "    startlearn = model_hyperparameters['learnrate']\n",
    "\n",
    "    # Only train the classifier (new_output) parameters, feature parameters are frozen\n",
    "    optimizer = optim.Adam(getattr(model, list(model._modules.items())[-1][0]).parameters(), lr=model_hyperparameters['learnrate'], weight_decay=model_hyperparameters['weightdecay'])\n",
    "\n",
    "    if type_loader == 'overfit_loader':\n",
    "        decay = 0.9 # hyperparameter decay factor for decaying learning rate\n",
    "        epoch = 200 # hyperparameter number of epochs\n",
    "\n",
    "    if type_loader == 'train_loader':\n",
    "        decay = 0.6 # hyperparameter decay factor for decaying learning rate\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model, ave_training_loss = o2_model_backprop(model, dict_data_loaders[type_loader], optimizer, criterion)\n",
    "        epoch_count_correct, ave_validate_loss = o3_model_no_backprop(model, dict_data_loaders['valid_loader'], criterion)\n",
    "\n",
    "        model_hyperparameters['training_loss_history'].append(ave_training_loss) # append ave training loss to history of training losses\n",
    "        model_hyperparameters['validate_loss_history'].append(ave_validate_loss) # append ave validate loss to history of validate losses\n",
    "\n",
    "        print('Epoch: {}/{}.. '.format(e+1, epoch),\n",
    "            'Train Loss: {:.3f}.. '.format(ave_training_loss),\n",
    "            'Valid Loss: {:.3f}.. '.format(ave_validate_loss),\n",
    "            'Valid Accuracy: {:.3f}.. '.format(epoch_count_correct / len(dict_data_loaders['valid_loader'].dataset)),\n",
    "            'Runtime - {:.0f} mins'.format((time.time() - t0)/60))\n",
    "\n",
    "        training_loss_history = model_hyperparameters['training_loss_history']\n",
    "        if len(training_loss_history) > 3: # hold loop until training_loss_history has enough elements to satisfy search requirements\n",
    "            if -3*model_hyperparameters['learnrate']*decay*decay*training_loss_history[0] > np.mean([training_loss_history[-2]-training_loss_history[-1], training_loss_history[-3]-training_loss_history[-2]]):\n",
    "                # if the average of the last 2 training loss slopes is less than the original loss factored down by the learnrate, the decay, and a factor of 3, then decay the learnrate\n",
    "                model_hyperparameters['learnrate'] *= decay # multiply learnrate by the decay hyperparameter\n",
    "                optimizer = optim.Adam(getattr(model, list(model._modules.items())[-1][0]).parameters(), lr=model_hyperparameters['learnrate'], weight_decay=model_hyperparameters['weightdecay']) # revise the optimizer to use the new learnrate\n",
    "                print('Learnrate changed to: {:f}'.format(model_hyperparameters['learnrate']))\n",
    "            if model_hyperparameters['learnrate'] <= startlearn*decay**(9*(decay**3)) and model_hyperparameters['running_count'] == 0: # super messy, I wanted a general expression that chose when to activate the deeper network and this worked\n",
    "                model = o4_control_model_grad(model, True)\n",
    "                model_hyperparameters['epoch_on'] = e\n",
    "                running = True # change the running parameter to True so that the future loop can start counting epochs that have run\n",
    "            if running: # if running, add to count for the number of epochs run\n",
    "                model_hyperparameters['running_count'] +=1\n",
    "            if running and model_hyperparameters['running_count'] > epoch/5: # deactivate parameters if running, add the count has reached its limiter\n",
    "                model = o4_control_model_grad(model, False)\n",
    "                running = False\n",
    "            if type_loader == 'overfit_loader':\n",
    "                if np.mean([training_loss_history[-1], training_loss_history[-2], training_loss_history[-3]]) < 0.0002:\n",
    "                    print('\\nModel successfully overfit images\\n')\n",
    "                    return model, model_hyperparameters\n",
    "                if e+1 == epoch:\n",
    "                    print('\\nModel failed to overfit images\\n')\n",
    "\n",
    "    return model, model_hyperparameters\n",
    "\n",
    "\n",
    "def o2_model_backprop(model, data_loader, optimizer, criterion):\n",
    "    # Check model can overfit the data when using a miniscule sample size, looking for high accuracy on a few images\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_train_loss = 0 # initialize total training loss for this epoch\n",
    "    model.train() # Set model to training mode to activate operations such as dropout\n",
    "    for images, labels in data_loader: # cycle through training data\n",
    "        images, labels = images.to(device), labels.to(device) # move data to GPU\n",
    "\n",
    "        optimizer.zero_grad() # clear gradient history\n",
    "        log_out = model(images) # run image through model to get logarithmic probability\n",
    "        loss = criterion(log_out, labels) # calculate loss (error) for this image batch based on criterion\n",
    "\n",
    "        loss.backward() # backpropogate gradients through model based on error\n",
    "        optimizer.step() # update weights in model based on calculated gradient information\n",
    "        epoch_train_loss += loss.item() # add training loss to total train loss this epoch, convert to value with .item()\n",
    "    ave_training_loss = epoch_train_loss / len(data_loader) # determine average loss per batch of training images\n",
    "\n",
    "    return model, ave_training_loss\n",
    "\n",
    "\n",
    "def o3_model_no_backprop(model, data_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    epoch_valid_loss = 0 # initialize total validate loss for this epoch\n",
    "    epoch_count_correct = 0 # initialize total correct predictions on valid set\n",
    "    model.eval() # set model to evaluate mode to deactivate generalizing operations such as dropout and leverage full model\n",
    "    with torch.no_grad(): # turn off gradient tracking and calculation for computational efficiency\n",
    "        for images, labels in data_loader: # cycle through validate data to observe performance\n",
    "            images, labels = images.to(device), labels.to(device) # move data to GPU\n",
    "\n",
    "            log_out = model(images) # obtain the logarithmic probability from the model\n",
    "            loss = criterion(log_out, labels) # calculate loss (error) for this image batch based on criterion\n",
    "            epoch_valid_loss += loss.item() # add validate loss to total valid loss this epoch, convert to value with .item()\n",
    "\n",
    "            out = torch.exp(log_out) # obtain probability from the logarithmic probability calculated by the model\n",
    "            highest_prob, chosen_class = out.topk(1, dim=1) # obtain the chosen classes based on greavalid calculated probability\n",
    "            equals = chosen_class.view(labels.shape) == labels # determine how many correct matches were made in this batch\n",
    "            epoch_count_correct += equals.sum()  # add the count of correct matches this batch to the total running this epoch\n",
    "\n",
    "        ave_validate_loss = epoch_valid_loss / len(data_loader) # determine average loss per batch of validate images\n",
    "\n",
    "    return epoch_count_correct, ave_validate_loss\n",
    "\n",
    "\n",
    "def o4_control_model_grad(model, control=False):\n",
    "    network_depth = len(list(model.children()))\n",
    "    param_freeze_depth = network_depth // 2\n",
    "    controlled_layers = []\n",
    "    layer_depth = 0\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer_depth += 1\n",
    "        if (network_depth - param_freeze_depth) < layer_depth < network_depth:\n",
    "            controlled_layers.append(layer._get_name())\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = control\n",
    "    print(f'\\n Toggle requires_grad = {control}: ', controlled_layers, '\\n')\n",
    "    return model\n",
    "\n",
    "\n",
    "def o5_plot_training_history(model_name, model_hyperparameters):\n",
    "    plt.clf()\n",
    "    plt.plot(model_hyperparameters['training_loss_history'], label='Training Training Loss')\n",
    "    plt.plot(model_hyperparameters['validate_loss_history'], label='Validate Training Loss')\n",
    "    if model_hyperparameters['epoch_on']:\n",
    "        plt.vlines(\n",
    "            colors = 'black',\n",
    "            x = model_hyperparameters['epoch_on'],\n",
    "            ymin = min(model_hyperparameters['training_loss_history']),\n",
    "            ymax = max(model_hyperparameters['training_loss_history'][3:]),\n",
    "            linestyles = 'dotted',\n",
    "            label = 'Deep Layers Activated'\n",
    "        ).set_clip_on(False)\n",
    "        plt.vlines(\n",
    "            colors = 'black',\n",
    "            x = (model_hyperparameters['epoch_on'] + model_hyperparameters['running_count']),\n",
    "            ymin = min(model_hyperparameters['training_loss_history']),\n",
    "            ymax = max(model_hyperparameters['training_loss_history'][3:]),\n",
    "            linestyles = 'dotted',\n",
    "            label = 'Deep Layers Deactivated'\n",
    "        ).set_clip_on(False)\n",
    "    plt.title(model_name)\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.xlabel('Total Epoch ({})'.format(len(model_hyperparameters['training_loss_history'])))\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f122f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cnn_model_functions import *\n",
    "\n",
    "# Store a dictionary of available models as names to avoid downloading models until a choice has been made\n",
    "model_name_dic = {'vgg': 'vgg16', 'alexnet': 'alexnet', 'googlenet': 'googlenet', 'densenet': 'densenet161',\n",
    "          'inception': 'inception_v3', 'resnext': 'resnext50_32x4d', 'shufflenet': 'shufflenet_v2_x1_0'}\n",
    "\n",
    "\n",
    "#Create a Classifier class, inheriting from nn.Module and incorporating Relu, Dropout and log_softmax\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc1 = nn.Linear(self.in_features, 512)\n",
    "        self.fc2 = nn.Linear(512, self.out_features)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def m1_create_classifier(model_name, classes_length):\n",
    "\n",
    "    #Download a pretrained convolutional neural network to reference, choose only the model requested by the user\n",
    "    model = getattr(models, model_name_dic[model_name])(pretrained=True)\n",
    "    print(model)\n",
    "    # Ensure that the in and out features for our model seamlessly match the in from the pretrained CNN and the out for the classes\n",
    "    # Rename the pretrained output layer to a default name 'new_output'\n",
    "    # pretrained_output_name = list(model._modules.items())[-1][0]\n",
    "    # model._modules['new_output'] = model._modules.pop(pretrained_output_name)\n",
    "    out_features = classes_length\n",
    "    in_features = list(model.children())[-1][1].weight.shape[1]\n",
    "\n",
    "#     model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "#     Freeze parameters so we don't backprop through them\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    setattr(model, list(model._modules.items())[-1][0], Classifier(in_features, out_features))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def m2_save_model_checkpoint(model, file_name_scheme, model_hyperparameters):\n",
    "    #Save the model state_dict\n",
    "    torch.save(model.state_dict(), file_name_scheme + '_dict.pth')\n",
    "    getattr(model, list(model._modules.items())[-1][0]).state_dict().keys()\n",
    "\n",
    "    #Create a JSON file containing the saved information above\n",
    "    with open(file_name_scheme + '_hyperparameters.json', 'w') as file:\n",
    "        json.dump(model_hyperparameters, file)\n",
    "\n",
    "\n",
    "def m3_load_model_checkpoint(model, file_name_scheme):\n",
    "    # Option to reload from previous state\n",
    "    checkpoint = torch.load(file_name_scheme + '_dict.pth')\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    with open(file_name_scheme + '_hyperparameters.json', 'r') as file:\n",
    "        model_hyperparameters = json.load(file)\n",
    "\n",
    "    print('loaded model learnrate = ', model_hyperparameters['learnrate'])\n",
    "\n",
    "    return model, model_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e93b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Classifier(\n",
       "    (fc1): Linear(in_features=9216, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_datasets, data_labels_dic = u2_load_processed_data(argdir)\n",
    "dict_data_loaders = u4_data_iterator(dict_datasets)\n",
    "\n",
    "model = m1_create_classifier(argmodel, len(dict_datasets['train_data'].classes))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19758f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential\n",
      "network_depth - param_freeze_depth = 1 < layer_depth = 1 < network_depth = 2\n",
      "Classifier\n",
      "network_depth - param_freeze_depth = 1 < layer_depth = 2 < network_depth = 2\n",
      "\n",
      " Toggle requires_grad = False:  [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Troubleshooter on param layer depth search\n",
    "network_depth = len(list(model.children()))\n",
    "param_freeze_depth = network_depth // 2\n",
    "control = False\n",
    "\n",
    "controlled_layers = []\n",
    "layer_depth = 0\n",
    "\n",
    "for layer in model.children():\n",
    "    print(layer._get_name())\n",
    "    layer_depth += 1\n",
    "    print(f'network_depth - param_freeze_depth = {network_depth - param_freeze_depth} < layer_depth = {layer_depth} < network_depth = {network_depth}')\n",
    "    if (network_depth - param_freeze_depth) < layer_depth < network_depth:\n",
    "        controlled_layers.append(layer._get_name())\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = control\n",
    "\n",
    "print(f'\\n Toggle requires_grad = {control}: ', controlled_layers, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_program_time = time.time()\n",
    "\n",
    "# Get processed data\n",
    "dict_datasets, data_labels_dic = u2_load_processed_data(argdir)\n",
    "dict_data_loaders = u4_data_iterator(dict_datasets)\n",
    "\n",
    "#Create file pathway for hyperparameter saving to JSON format later\n",
    "file_name_scheme = os.path.basename(os.path.dirname(argdir)) + '_' + argmodel\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Download a classifer model for use\n",
    "model = m1_create_classifier(argmodel, len(dict_datasets['train_data'].classes))\n",
    "\n",
    "# if argtrain == 'y':\n",
    "# if u5_time_limited_input('Check model can overfit small dataset'):\n",
    "overfit_model, model_hyperparameters = o1_train_model(model, dict_data_loaders, argepoch, 'overfit_loader', criterion)\n",
    "o5_plot_training_history(argmodel, model_hyperparameters)\n",
    "plt.savefig(file_name_scheme + '_training_history_overfit.png')\n",
    "print('Saved overfit training history to project directory')\n",
    "\n",
    "# if u5_time_limited_input('Continue with complete model training'):\n",
    "#     model, model_hyperparameters = o1_train_model(model, dict_data_loaders, argepoch, 'train_loader', criterion)\n",
    "#     o5_plot_training_history(argmodel, model_hyperparameters)\n",
    "#     plt.savefig(file_name_scheme + '_training_history_complete.png')\n",
    "#     print('Saved complete training history to project directory')\n",
    "\n",
    "#     if u5_time_limited_input('Would you like to test the model'):\n",
    "t1 = time.time()\n",
    "test_count_correct, ave_test_loss = o3_model_no_backprop(model, dict_data_loaders['testing_loader'], criterion)\n",
    "print('testing Loss: {:.3f}.. '.format(ave_test_loss),\n",
    "    'testing Accuracy: {:.3f}'.format(test_count_correct / len(dict_data_loaders['testing_loader'].dataset)),\n",
    "    'Runtime - {:.0f} seconds'.format((time.time() - t1)))\n",
    "\n",
    "#     #Save the model hyperparameters and the locations in which the CNN training activated and deactivated\n",
    "#     if u5_time_limited_input('Would you like to save the model'):\n",
    "#         m2_save_model_checkpoint(model, file_name_scheme, model_hyperparameters)\n",
    "\n",
    "# if argtrain == 'n':\n",
    "#     model, model_hyperparameters = m3_load_model_checkpoint(model, file_name_scheme)\n",
    "#     o5_plot_training_history(argmodel, model_hyperparameters)\n",
    "#     plt.show(block=False)\n",
    "#     plt.pause(3)\n",
    "#     plt.close()\n",
    "\n",
    "#     learnrate = model_hyperparameters['learnrate']\n",
    "#     training_loss_history = model_hyperparameters['training_loss_history']\n",
    "#     validate_loss_history = model_hyperparameters['validate_loss_history']\n",
    "#     epoch_on = model_hyperparameters['epoch_on']\n",
    "#     running_count = model_hyperparameters['running_count']\n",
    "\n",
    "#     print('The model is ready to provide predictions')\n",
    "\n",
    "    #\n",
    "    # o6_predict_data():\n",
    "    #\n",
    "    # u7_show_prediction():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
